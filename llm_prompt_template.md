System persona: 
 Ты – senior python developer, эксперт по работе с последней версией Langchain, который знает и умеет применять
лучшие практики в работе с LLM. https://docs.langchain.com/oss/python/langchain/overview


Context:
Описание артефакта: Мы разработаем скрипт style_eval.py, который принимает на вход набор ответов (например, лог диалога или тестовые фразы) и проверяет каждый с помощью комбинации правил и LLM-оценки. Результат – файл style_eval.json, содержащий подробности по каждому проверенному ответу и суммарную статистику.

Основные составляющие скрипта:

Набор тестовых запросов и ответов для проверки. Например, можно взять 10 типичных пользовательских вопросов и сгенерировать на них ответы текущей моделью (или использовать реальные примеры диалогов).

Правила стиля – функции или регулярные выражения, которые выявляют отклонения (определяются на основе бренд-гайдов).

LLM для оценки – здесь используем GPT-4o как высокоточную модель оценки стиля (чтобы не тратить ресурс GPT-5, или наоборот GPT-5 если нам нужна лучшая оценка). Ей будем отправлять короткий промпт с просьбой оценить ответ по нескольким критериям.

Формирование отчёта – собираем всё в структуру данных и сохраняем в JSON.

Пример реализации ключевых частей:

from langchain_ollama import ChatOllama
import json
import os
load_dotenv()

client = ChatOllama(model="gpt-oss:120b-cloud", temperature=0.0)

test_cases = [
    {"user": "Здравствуйте, у меня проблема с заказом.", 
     "assistant": "Привет! Давай посмотрим, что случилось с твоим заказом."},
    {"user": "Добрый день. Не могли бы вы подсказать статус заявки 324?", 
     "assistant": "Добрый день! С удовольствием помогу вам узнать статус вашей заявки №324."},
    {"user": "Чё по доставке, где мой товар?", 
     "assistant": "Добрый день. Пожалуйста, уточните номер вашего заказа, чтобы я мог проверить информацию."}
]

def style_rules_check(text: str) -> dict:
    issues = {}
    if " ты " in text.lower() or text.lower().startswith("ты"):
        issues["second_person_informal"] = True
    if "чё" in text.lower():
        issues["slang_detected"] = True
    return issues

evaluation_prompt = (
    "Ты — эксперт по стилю текста ассистента. Проанализируй ответ ассистента и оцени, соблюдает ли он требования:\n"
    "1) Тон официально-вежливый (да/нет);\n2) Нет ли сленга или жаргона (да/нет);\n3) Обращение на Вы (да/нет).\n"
    "Если есть нарушения, перечисли какие.\n\n"
    "Ответ ассистента: \"{answer}\"\n"
    "Вывод:"
)

def evaluate_with_llm(answer: str) -> dict:
    prompt = evaluation_prompt.format(answer=answer)
    try:
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=[{"role": "user", "content": prompt}],
            temperature=0
        )
        eval_text = response.choices[0].message.content
    except Exception as e:
        eval_text = f"Ошибка при оценке LLM: {e}"
    return {"evaluation": eval_text.strip()}

                  
В этом коде мы настроили промпт так, чтобы GPT-4o ответил, соблюдается стиль или нет, и указал нарушения. Поскольку ответ LLM может быть в произвольной форме (например, «1) да; 2) нет, использовано слово "чё"; 3) нет, обращение на ты»), мы можем либо парсить текст, либо вернуть сырой текст в отчёт (на первых порах можно и так, для наглядности).

Теперь, пробежимся по тестовым случаям, применим правила и LLM-оценку, и соберём отчёт:

 
report = {"cases": []}
summary_violations = {"second_person_informal": 0, "slang_detected": 0}

for case in test_cases:
    assistant_answer = case["assistant"]
    rule_issues = style_rules_check(assistant_answer)
    for issue in rule_issues:
        if rule_issues[issue]:
            summary_violations[issue] = summary_violations.get(issue, 0) + 1
    llm_eval = evaluate_with_llm(assistant_answer)
    case_report = {
        "user": case["user"],
        "assistant": assistant_answer,
        "rule_issues": rule_issues,
        "llm_evaluation": llm_eval["evaluation"]
    }
    report["cases"].append(case_report)

report["summary"] = {
    "total_cases": len(test_cases),
    "violations": summary_violations
}

with open("style_eval.json", "w", encoding="utf-8") as f:
    json.dump(report, f, ensure_ascii=False, indent=2)
print("Отчёт сохранён в style_eval.json")

                  
После запуска style_eval.py получим файл style_eval.json. Примерный содержимое (отформатировано для читабельности):

 
{
  "cases": [
    {
      "user": "Здравствуйте, у меня проблема с заказом.",
      "assistant": "Привет! Давай посмотрим, что случилось с твоим заказом.",
      "rule_issues": {
        "second_person_informal": true,
        "slang_detected": false
      },
      "llm_evaluation": "1) Тон неофициальный (обращение на \"ты\"), 2) сленга нет, 3) на \"Вы\" не обращается."
    },
    {
      "user": "Добрый день. Не могли бы вы подсказать статус заявки 324?",
      "assistant": "Добрый день! С удовольствием помогу вам узнать статус вашей заявки №324.",
      "rule_issues": {
        "second_person_informal": false,
        "slang_detected": false
      },
      "llm_evaluation": "1) Тон вежливый, 2) сленга нет, 3) обращение корректное на \"вы\" (хотя написано с маленькой буквы, это допустимо в тексте)."
    },
    {
      "user": "Чё по доставке, где мой товар?",
      "assistant": "Добрый день. Пожалуйста, уточните номер вашего заказа, чтобы я мог проверить информацию.",
      "rule_issues": {
        "second_person_informal": false,
        "slang_detected": false
      },
      "llm_evaluation": "1) Тон вежливый, официальный, 2) сленга нет (ассистент сам не использует сленг, хотя пользователь использовал), 3) обращение на \"вы\"."
    }
  ],
  "summary": {
    "total_cases": 3,
    "violations": {
      "second_person_informal": 1,
      "slang_detected": 0
    }
  }
}

                  
В данном отчёте мы видим, что в первом ответе ассистент нарушил стиль (обратился «Привет» на ты, что не соответствует официальному тону). Это отражено и правилом (second_person_informal = true), и LLM-оценкой (которая заметила обращение на "ты"). Во втором и третьем случаях нарушений нет. Сводная статистика summary показывает, что из 3 ответов 1 содержит обращение на "ты", сленг не обнаружен ни в одном. Этот артефакт style_eval.json поможет нам понять, где модель отклоняется от гайдлайнов.

Как использовать отчёт: На основе подобного отчёта команда разработчиков или контент-редакторов может решить, что нужно скорректировать. В нашем примере видно: ассистент в неформальной речи пользователя тоже сначала ответил неформально («Привет!») – возможно, систему надо настроить, чтобы всегда сохранять официальный тон независимо от тона пользователя. То есть, внести поправку в system prompt: «Никогда не используешь разговорных сокращений, даже если пользователь их употребляет.» или включить пример, где пользователь грубит, а ассистент отвечает вежливо. Повторный запуск оценки покажет, исправилась ли ситуация.

Лучшие практики при генерации style-отчётов:

Убедитесь, что проверяющая LLM не путается: задавайте ей вопросы в явном формате (как мы сделали нумерованный список критериев).

Если объём ответов большой, можно генерировать отчёт выборочно, или использовать более дешёвые модели для части критериев. Например, детектировать специфические слова можно и без GPT.

Авто-отчёт не заменяет финальной ручной проверки наиболее критичных случаев – он служит фильтром, сигнализирующим о проблемах.

Включайте в отчёт не только нарушения, но и позитивные моменты: например, процент ответов, где полностью соблюдены все критерии. Это важно для метрик качества обслуживания.

Используйте версионирование: если вы обновили промпт/модель, снимите новый отчёт style_eval.json и сравните с предыдущим – прогресс ли, не появились ли новые проблемы.

Task:
 Создать и продемонстрировать работу инструмента, который автоматически оценивает стиль ответов LLM и формирует отчёт (в формате JSON) с найденными нарушениями и оценками.
Исправить не соответствия на использование ChatOllama


Constraint: 
    - отвечай только на тот вопрос, что тебя попросили 
    - не добавляй в ответ, ничего лишнего 
    - не создавай примеров использования, readme и прочего, если тебя явно об этом не попросили
    - НИКОГДА не пиши демонстрационных примеров, примеров использования, инструкций и readme, пока я сам тебя об этом не попрошу 
