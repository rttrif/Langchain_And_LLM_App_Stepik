System persona: 
 Ты – senior python developer, эксперт по работе с последней версией Langchain, который знает и умеет применять
лучшие практики в работе с LLM. https://docs.langchain.com/oss/python/langchain/overview

System tone:
Отвечай коротко и по сути, о чем тебя попросили. Если не знаешь или не уверен, то так и скажи. 

Context:
Я изучаю Langchain на курсе, но он устаревший в нем есть такое описание 

3. Использование готовой цепочки RetrievalQA
LangChain упрощает создание RAG-процесса с помощью класса RetrievalQA. Он автоматически выполняет: поиск документов + формирование вопроса с контекстом + получение ответа.

 

Простой способ:

from langchain.chains import RetrievalQA

qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",         # метод комбинации документов: "stuff" = просто вставить все
    retriever=retriever,
    return_source_documents=True   # чтобы потом увидеть, какие документы использованы (для анализа)
)

                  
Аргумент chain_type может быть:

"stuff" – самый прямой: просто берет все найденные docs, “напихивает” (stuff) их в промпт перед вопросом.

"map_reduce" – более сложный: отправляет каждый документ в модель, получает отдельные промежуточные ответы, затем сводит их (например, еще раз моделью) к одному. Это полезно, если документов очень много или текст в них длинный – модель обрабатывает по частям. Но работает дольше и сложнее устроен.

"refine" – тоже итеративный подход: берет первый документ, генерирует черновой ответ, потом по очереди каждый следующий документ уточняет ответ (модель получает предыдущий ответ + новый документ и может переписать ответ).

"map_rerank" – модель оценивает каждый документ на релевантность ответу.
Для начала нам хватит "stuff", потому что у нас немного документов и они не огромные (мы же их ограничили чанкингом).

Теперь, имея qa_chain, мы можем просто вызывать:

 
result = qa_chain({"query": query})
answer = result["result"]
source_docs = result["source_documents"]

                  
Если return_source_documents=True, то вернется словарь с ключами result (ответ) и source_documents (список документов). Если False, просто qa_chain.run(query) вернет только строку ответа.

 

Пример работы цепочки:

query = "Когда была основана компания и кем?"
result = qa_chain({"query": query})
print("Ответ:", result["result"])
for i, doc in enumerate(result["source_documents"], 1):
    print(f"Источник {i}: {doc.metadata.get('source')} стр.{doc.metadata.get('page')}")

                  
Предположим, ответ модели: "Компания была основана в 1999 году основателями Иваном Ивановым и Марией Петровой." Это правильный ответ, полученный за счет того, что среди source_documents был кусок с этой информацией.

 

Как цепочка формирует промпт? По умолчанию RetrievalQA (chain_type="stuff") использует следующий шаблон промпта (примерно, на английском в оригинале):

 
Use the following pieces of context to answer the question.
If you don't know the answer, just say you don't know, don't try to make up an answer.

{context}

Question: {query}
Helpful Answer:

                  
То есть, она вставляет контекст (конкатенация всех найденных документов, возможно с нумерацией или разделителями) и затем вопрос. Модель должна выдать полезный ответ. В LangChain шаблон можно поменять через параметр chain_type_kwargs или создав PromptTemplate, но пока можно использовать стандартный. Важная часть: в шаблоне есть инструкция не выдумывать ответа, если не знает. Это хорошая защита от галлюцинаций: лучше модель ответит "не знаю", чем нафантазирует. Вы можете локализовать этот промпт на русский, но часто даже английский работает (модель видит русские контексты и вопрос, и отвечает по-русски; инструкции может понять на англ). Тем не менее, для четкости можно задать свой промпт на русском.

 

Кастомизация промпта с цитированием источников: В нашем курсе мы хотим, чтобы модель приводила ссылки на источники в ответе. По умолчанию, RetrievalQA этого не делает явно. Мы можем модифицировать шаблон, добавив инструкцию: например: "Ответь на вопрос, используя предоставленные данные. В конце каждого факта укажи в скобках номер источника, откуда взята информация."

 

Для этого создадим PromptTemplate:

from langchain.prompts import PromptTemplate

template = """Используй следующий контекст для ответа на вопрос. Если ответ не находится в контексте, ответь "Не знаю". Отвечай развернуто и по делу. По возможности цитируй данные из источников. После приведенных фактов указывай в квадратных скобках номер источника.

{context}

Вопрос: {question}
Ответ:"""

QA_PROMPT = PromptTemplate(template=template, input_variables=["context", "question"])

                  
Здесь мы говорим модели цитировать источники в формате [1], [2], ... (номера источников). Теперь при создании RetrievalQA можно указать свой промпт через prompt или chain_type_kwargs. Однако, для chain_type="stuff" простого параметра нет, он берет либо дефолт, либо надо через CombineDocumentsChain. Проще: мы сами сделаем генерацию:

 
docs = retriever.get_relevant_documents(query)
context = ""
for idx, doc in enumerate(docs, 1):
    context += f"[{idx}] {doc.page_content}\n"
prompt = QA_PROMPT.format(context=context, question=query)
answer = llm.predict(prompt)  # или llm.call_as_llm if ChatOpenAI

                  
Здесь мы нумеруем документы и составляем строку контекста, потом форматируем промпт. Метод llm.predict для ChatOpenAI выполняет один разворот (можно еще использовать llm как Callable).

 

Проверка цитирования: Модель GPT-4 обычно понимает инструкцию и поставит [1] или [2] после фактов. Но нужно убедиться, что она правильно сопоставляет номер с источником. В нашем коде мы выдали документы в порядке 1..N, и она может сослаться, например: "...в 1999 году [1]", что означает, что по ее мнению в документе [1] (вероятно первом) это написано. Обычно это так, ведь мы дали текст документа прямо. Но GPT может ошибиться или сослаться не на тот. Это отдельная проблема – проверка достоверности цитат (о ней поговорим в уроке 19).

 

Режим диалога (ConversationalRetrievalChain): Если вам нужно поддерживать диалог с памятью (последовательность вопросов, где пользователь задает уточнения), LangChain предлагает ConversationalRetrievalChain. Он сохраняет историю диалога и может добавлять ее в контекст, плюс может использовать ее при поиске (например, брать тему из предыдущих вопросов). Это продвинутая тема; в данном уроке мы ориентируемся на одиночные вопросы, но имейте в виду: если вы строите чат-бота, который должен помнить разговор, нужно хранить историю (например, ограниченную) и все равно на каждом новом вопросе выполнять retrieval.

Task:
Я подготовил файл, изучи контекст и скажи как мне получить аналогичный результат в последней версии фреймворка используя лушие практики

Как устранить ошибку 


Constraint: 
    - отвечай только на тот вопрос, что тебя попросили 
    - не добавляй в ответ, ничего лишнего 
    - не создавай примеров использования, readme и прочего, если тебя явно об этом не попросили
    - НИКОГДА не пиши демонстрационных примеров, примеров использования, инструкций и readme, пока я сам тебя об этом не попрошу 
    - Используй последнюю версию документации Langchain 
