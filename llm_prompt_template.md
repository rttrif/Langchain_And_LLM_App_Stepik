System persona: 
 Ты – senior python developer, эксперт по работе с последней версией Langchain, который знает и умеет применять
лучшие практики в работе с LLM. https://docs.langchain.com/oss/python/langchain/overview

System tone:
Отвечай коротко и по сути, о чем тебя попросили. Если не знаешь или не уверен, то так и скажи. 

Context:
Шаг 1. Автоматическая оценка стиля ответов
Цель: Научиться использовать возможности самой LLM для оценки стиля генерируемых ответов. Идея в том, что языковая модель (например, GPT-5) может выступать в роли "судьи" и оценивать ответы по заданным критериям стиля – вместо трудоёмкой ручной проверки. Это позволяет масштабно проверять соответствие ответов требуемому тону, формату и другим субъективным критериям.

Объяснение: Традиционные метрики (например, точность совпадения с образцом) плохо подходят для оценки стилистических качеств или тона ответа, поскольку стиль – вещь субъективная. Люди способны оценивать нюансы тона, формальности, дружелюбности, но привлечение человека к проверке каждого ответа не масштабируется. Решение – LLM-as-a-Judge, когда сама модель оценивает текст по нашим критериям. Мы формируем специальный оценочный промпт, описывающий желаемый стиль, и просим модель проанализировать ответ: соответствует ли он требованиям по стилю, тону, форматированию и т.д., возможно, выставить баллы или категории. Пример: если требование – “ответ должен быть уважительным, техническим и не слишком разговорным”, модель-оценщик читает сгенерированный ответ и решает, соблюдены ли эти аспекты.

Инженерные приёмы: При реализации LLM-оценки стиля важно чётко сформулировать критерии в промпте. Рекомендуется перечислить конкретные аспекты: например, вежливость, формальность речи, употребление профессиональной терминологии, отсутствие жаргона и т.п. Можно попросить модель вернуть структурированный вывод – например, JSON с оценками по каждому критерию. Это повысит объективность и удобство последующей обработки. Также полезно использовать принцип “chain-of-thought” (если модель позволяет) – т.е. чтобы модель сперва размышляла над соответствием критериям, а затем выдала итоговую оценку. Либо можно сразу запросить только итог без объяснений, если важна краткость. На практике, LLM-оценка гибка: можно настроить её под любые пользовательские стилистические гайды. В LangChain или через OpenAI API можно осуществить этот вызов так же, как генерацию ответа, только промпт будет оценочным.

Лучшие практики:

Ясные критерии: в явном виде включите в промпт список правил стиля или даже пример правильного и неправильного ответа, чтобы LLM-оценщик понимал ваши стандарты.

Ограничение субъективности: попросите модель давать количественную оценку (например, шкала 1–5) или бинарное соответствие по каждому критерию, чтобы упростить автоматическую интерпретацию.

Валидация: периодически проверяйте несколько оценок моделью вручную или с коллегой, чтобы убедиться, что LLM не «съехала» и понимает критерии правильно. Помните, что LLM может быть предвзята или ошибаться, поэтому критически оценивайте её вердикты.

Избегайте утечки ответа в оценку: если вы сравниваете два ответа (например, A/B), можно применить pairwise сравнение, но чаще нам достаточно независимой оценки единственного ответа по стилю.

Пример кода: Ниже приведён упрощённый пример, как с помощью API EssayAI (модели GPT-5) получить оценку стиля ответа. Предположим, у нас есть некоторое содержательное answer_text и мы хотим проверить, соответствует ли он формальному деловому стилю и не содержит ли личных местоимений от первого лица. Мы сформируем системное сообщение с инструкциями для оценщика, а в пользовательское сообщение поместим проверяемый ответ. Модель вернёт оценку и комментарии:

import openai

# Настройка API EssayAI для вызова GPT-5 через OpenAI SDK
openai.api_base = "https://api.essayai.ru/v1"
openai.api_key = "<ВАШ_API_КЛЮЧ>"

system_message = {
    "role": "system",
    "content": (
        "Ты – помощник, оценивающий стиль ответа по заданным правилам. "
        "Критерии: (1) тон ответа – деловой, вежливый; (2) речь от третьего лица, "
        "избегать местоимения 'я'; (3) нет жаргона, только профессиональные термины. "
        "Ответь в формате JSON с полями: formality_score (0-10), no_first_person (True/False), comment."
    )
}
user_message = {
    "role": "user",
    "content": f"Оцени стиль следующего ответа:\n\"\"\"\n{answer_text}\n\"\"\""
}

response = openai.ChatCompletion.create(
    model="gpt-5", 
    messages=[system_message, user_message]
)
result = response['choices'][0]['message']['content']
print(result)
# Пример вывода (JSON строка):
# {"formality_score": 8, "no_first_person": false, "comment": "Ответ в целом формальный, но присутствует 'я думаю', что нарушает правило обращения от третьего лица."}

                  
Пояснения к примеру: Мы используем openai.ChatCompletion с базовым URL EssayAI. Модель GPT-5 получает системное сообщение с описанием критериев оценки. В этом примере мы просим вернуть JSON с оценкой формальности по десятибалльной шкале, флагом нарушения правила про первое лицо и кратким комментарием. Модель читает answer_text и генерирует оценку. В выводе (показан пример) видно, что модель присвоила довольно высокий формальный стиль (8 из 10), но флаг no_first_person ложный (т.е. она обнаружила местоимение первого лица) и добавила комментарий, что присутствует фраза "я думаю", нарушающая требование.

Требования к артефактам: На этом шаге артефакта ещё нет – мы получаем промежуточный результат (оценку стиля конкретного ответа). Однако важно продумать, как будет выглядеть финальный отчёт: какие поля в JSON необходимы, какие метрики или выводы нужны заинтересованным лицам. Если, к примеру, в курсе требовалось оценивать стиль каждого ответа системы на тестовом наборе вопросов, то нужно предусмотреть, что отчёт будет содержать список оценок по каждому ответу. Далее, на шаге 3, мы займёмся формированием полного отчёта. Сейчас же убедитесь, что вывод модели структурирован (например, JSON как в примере) – это облегчит объединение данных.

Шаг 2. Статические правила (lint для ответов)
Цель: Научиться дополнять субъективную оценку LLM объективной проверкой ряда фиксированных правил (статическим анализом ответа). Это своего рода “линтер” для текстовых ответов, который автоматически находит нарушения явно определённых требований к стилю. Такие проверки работают по принципу шаблонов/регулярных выражений или простых функций без участия модели.

Объяснение: Статические правила – это формализованные требования к ответу, которые можно проверить алгоритмически. Например, максимальная длина ответа (не более N символов или предложений), отсутствие определённых запрещённых фраз, соответствие формату (скажем, ответ должен содержать слово "Итого:" в конце при вычислении, или не должен обращаться к пользователю на "ты"). Эти вещи не требуют понимания смысла – достаточно простого поиска или подсчёта. Использование таких правил дополняет оценку LLM: модель может упустить некоторые конкретные моменты или быть непоследовательной, а вот скрипт-линтер надёжно сигнализирует о формальных нарушениях. К тому же, запуск правил дешевле, чем вызов LLM, и можно проверить ими все ответы, не расходуя лимит модели.

Инженерные приёмы: Реализация статических проверок обычно сводится к написанию набора функций или одного скрипта, последовательно применяющего правила к тексту ответа. Примеры техник:

Регулярные выражения (regex): для поиска запрещённых конструкций. Например, re.search(r"\bя думаю\b", text, re.IGNORECASE) обнаружит фразу "я думаю".

Преобразование регистра и символов: для более простой проверки можно приводить текст к одному регистру, удалять знаки препинания временно, чтобы не пропустить вариант написания.

Парсинг структуры: если формат строго структурирован (скажем, JSON или список), можно попытаться распарсить и проверить наличие/отсутствие полей.

Подсчёт количества предложений/слов: можно разбить текст по точкам или пробелам и проверить длину (например, не более 3 предложений, или не менее 50 слов).

Словари стоп-слов или нежелательных слов: составить список фраз, которые не должны встречаться (например, жаргон, неформальные обращения вроде "чувак", эмодзи и т.п.) и проверить вхождение.

В контексте стиля, примеры правил:

Не использовать местоимения 1-го лица ("я", "мы").

Не обращаться к пользователю на "ты" (то есть отсутствие "ты", "тебе" и т.п., если стиль требует формального "вы" или безличного изложения).

Ограничение максимального числа восклицательных знаков (эмоциональность).

Если требуем определённый формат (например, ответ должен начинаться с краткого вывода), проверить начальное предложение.

Правописание: можно подключить библиотеку проверки орфографии/грамматики (например, LanguageTool), но это уже динамически, хотя её можно считать частью статических проверок.

Лучшие практики:

Документирование правил: Убедитесь, что все правила явно перечислены и понятны. Внутри кода оставьте комментарии, что проверяется, либо вынесите описания в конфигурацию (например, YAML/JSON с правилами).

Гибкость и настройка: Хорошо, если правила легко включать/отключать или менять пороги. Например, длину ответа лучше сделать параметром.

Логирование нарушений: Полезно не только флагнуть, что правило нарушено, но и записать, где именно. Например, правило "нет 'я'" – хорошо бы указать, на каком слове споткнулись. Это может включаться в отчёт или хотя бы в консольный вывод при прогоне.

Не дублировать то, что уже проверяет LLM: Если LLM-оценка и так ставит балл за формальность, нет смысла вторым способом пытаться измерять формальность. Правила должны ловить то, что легко формализовать и однозначно трактуется.

Пример кода: Реализуем небольшой набор статических проверок на Python. Допустим, у нас есть ответ answer_text. Напишем функции для пары правил: (1) отсутствие местоимения "я" вне цитат, (2) максимальное число предложений – не более 3. Используем re для поиска и простой разбиватель по точкам для подсчёта предложений:

import re

def check_no_first_person(text: str) -> bool:
    """Правило: ответ не должен содержать 'я ' или 'мы ' как самостоятельные слова (возвращает True, если правило выполнено)."""
    # Игнорируем регистр, ищем ' я ' или начало строки 'я ' или ' мы ' 
    # Используем \b для границ слова, добавляем пробел или пунктуацию для точности.
    return (re.search(r"\bя\b", text, flags=re.IGNORECASE) is None 
            and re.search(r"\bмы\b", text, flags=re.IGNORECASE) is None)

def check_sentence_count(text: str, max_sentences: int = 3) -> bool:
    """Правило: количество предложений не превышает max_sentences."""
    # Грубый способ: разбить по точкам, восклицательным и вопросительным знакам.
    sentences = re.split(r"[.!?]+", text)
    # Убираем пустые фрагменты от split и считаем
    num_sentences = sum(1 for s in sentences if s.strip())
    return num_sentences <= max_sentences

# Пример использования:
violations = {}
violations['no_first_person'] = not check_no_first_person(answer_text)
violations['too_many_sentences'] = not check_sentence_count(answer_text, max_sentences=3)

print(violations)
# Например вывод: {'no_first_person': True, 'too_many_sentences': False}
# Это значит: нарушено правило no_first_person (обнаружены "я" или "мы"), правило по количеству предложений соблюдено.

                  
В данном примере violations – словарь, где мы отмечаем нарушение (True значит правило нарушено, False – нет). Если answer_text = "Я считаю. Это важно.", то no_first_person будет True (нарушено, так как есть "Я"), а too_many_sentences – False (предложений 2, это не превышает лимит 3). В реальной ситуации подобных правил может быть много, и их стоит организовать структурно – например, списком правил и циклом по ним.

Требования к артефактам: На этом шаге артефактом также не является что-то осязаемое, кроме написанного кода правил. В конечном отчёте style_eval.json (шаг 3) результаты этих проверок появятся, поэтому важно, чтобы код правил выдавал понятные результаты (например, список нарушенных правил или их отсутствие). При оформлении артефакта (скрипта отчёта) убедитесь, что набор правил легко расширяем – возможно, вынесите их в конфиг, чтобы в будущем добавлять новые требования без изменения логики программы.

Шаг 3. Формирование отчёта style_eval.json
Цель: Собрать результаты оценки стиля (как от LLM, так и от статического линтера) в единый структурированный отчёт в формате JSON. Это нужно, чтобы представить итоговую оценку стиля для набора ответов – например, всех ответов системы на тестовые вопросы. Отчёт style_eval.json позволит команде проанализировать, насколько хорошо система соблюдает заданный стиль во всех ответах, и увидеть конкретные нарушения.

Объяснение: Отчёт представляет собой файл JSON, содержащий для каждого проверенного ответа сводку по его стилю. Структура может быть, например:

{
  "answers": [
    {
      "id": 1,
      "text": "<текст ответа>",
      "llm_scores": { ... }, 
      "violations": { ... }
    },
    ...
  ],
  "summary": {
     "total_answers": N,
     "violations_count": { "no_first_person": X, "too_many_sentences": Y, ... },
     "average_formality_score": Z
  }
}

                  
Здесь для каждого ответа сохраняется текст (или хотя бы идентификатор, если текст длинный), результаты LLM-оценки (llm_scores – например, баллы за формальность, уместность тона и т.д., полученные на шаге 1) и результаты статических проверок (violations – что нашли на шаге 2). Раздел summary даёт агрегированную статистику: сколько ответов проверено, сколько нарушений каждого типа и средний балл или рейтинг по критериям. Такая структура удобна – можно быстро понять, какие правила чаще всего нарушаются, средний уровень соответствия стилю и т.п.

 
Процесс формирования отчёта:

Для каждого ответа в тестовом наборе вызываем LLM-оценку (шаг 1) и линтер (шаг 2).

Сохраняем результаты в Python-объект (например, список словарей).

Рассчитываем сводные показатели (агрегируя по всем ответам).

Записываем всё в файл style_eval.json.

Инженерные приёмы:

Параллельная обработка: Если ответов много, можно вызывать оценку LLM параллельно (с учётом ограничений API). Например, с помощью многопоточного пула или асинхронно, чтобы сократить время.

Кэширование LLM-оценки: Чтобы не платить дважды, если один и тот же ответ оценивается многократно при отладке, имеет смысл реализовать простое кэширование (например, хранить уже полученную оценку по хэшу ответа).

Валидация JSON от LLM: Поскольку мы просили LLM вернуть JSON, убедитесь, что он парсится без ошибок. Иногда модель может вокруг JSON добавить комментарии или извинения – нужно обработать эти случаи. Можно применять трюк: попросить модель выводить только JSON без пояснений, или использовать regex/парсер для извлечения JSON-субстроки. LangChain, кстати, предоставляет OutputParser для JSON, который может помочь извлечь JSON из сырого ответа модели.

Добавление идентификаторов: В отчетах удобно нумеровать ответы или привязывать к ID вопроса, чтобы потом легко найти оригинал ответа. Если у вас тестовый набор вопросов, присвойте каждому вопросу/ответу ID и используйте его в отчёте.

Читаемость: Хотя JSON – формат для машины, делайте его в человеко-читаемом виде (например, отступы, возможно, сортировка ключей). Так, при отладке или просмотре отчёта в репозитории будет легче анализировать.

Лучшие практики:

Согласованность метрик: Убедитесь, что интерпретация оценок LLM и правил понятна. Например, если LLM возвращает formality_score 8 из 10, а правило no_first_person нарушено, то при анализе эти данные должны дополнять друг друга, а не противоречить. В данном примере высокие баллы формальности сочетаются с нарушением отсутствия первого лица – это можно объяснить, т.е. не конфликт. Но если бы LLM дала 10/10 по формальности, а правило нашло “я”, это несостыковка – возможно, стоит пересмотреть либо критерии, либо порог восприятия LLM.

Автоматический анализ: Раз уж отчёт в JSON, подумайте о дальнейших шагах: скрипт мог бы сразу подсвечивать "красные зоны" – например, печатать в консоль итоги: N ответов из M не прошли линтер, чаще всего нарушено правило X. Такие выводы помогут сконцентрироваться на проблемах.

Версионирование отчёта: Нередко такие отчёты полезно сравнивать по разным версиям модели или промпта. Включите в файл версию модели (например, GPT-5 vs GPT-4o), дату генерации, версию вашего стилевого гайда. Тогда через месяц, глядя на файл, будет понятно контекст, при котором он получен.

Пример кода: Предположим, у нас есть список ответов answers = ["...ответ1...", "...ответ2...", ...]. Покажем фрагмент формирования отчёта с использованием ранее написанных функций. Здесь мы для простоты вызываем синхронно, без параллелизма:

import json

answers = [
    {"id": 1, "text": "Здравствуйте! Я рад помочь. Вот решение..."},
    {"id": 2, "text": "Привет, думаю, тебе стоит попробовать это..."}
    # ... далее остальные ответы ...
]

report_data = {"answers": [], "summary": {}}
# Счётчики для summary
total = 0
violations_count = {"no_first_person": 0, "too_many_sentences": 0}
formality_scores_sum = 0

for ans in answers:
    total += 1
    text = ans["text"]
    # 1. Получаем LLM-оценку стиля (используем функцию/способ из шага 1)
    style_eval_json = get_style_evaluation_from_llm(text)  # предполагается, что эта функция вызовет GPT-5 и вернёт dict
    # 2. Проверяем статические правила (из шага 2)
    viol = {
        "no_first_person": not check_no_first_person(text),
        "too_many_sentences": not check_sentence_count(text, max_sentences=3)
    }
    # 3. Собираем данные по этому ответу
    answer_entry = {
        "id": ans["id"],
        "text": text,
        "llm_scores": style_eval_json,       # например, {"formality_score": 8, ...}
        "violations": viol                   # например, {"no_first_person": true, "too_many_sentences": false}
    }
    report_data["answers"].append(answer_entry)
    # 4. Обновляем summary-данные
    formality_scores_sum += style_eval_json.get("formality_score", 0)
    if viol["no_first_person"]: violations_count["no_first_person"] += 1
    if viol["too_many_sentences"]: violations_count["too_many_sentences"] += 1

# 5. Заполняем summary
report_data["summary"] = {
    "total_answers": total,
    "violations_count": violations_count,
    "average_formality_score": round(formality_scores_sum / total, 2) if total else None
}

# 6. Сохраняем в JSON-файл
with open("style_eval.json", "w", encoding="utf-8") as f:
    json.dump(report_data, f, ensure_ascii=False, indent=2)

print("Отчёт style_eval.json сгенерирован для", total, "ответов.")

                  
В этом коде функция get_style_evaluation_from_llm должна быть реализована вами – это обёртка над вызовом GPT-5, возвращающая dict с оценками (по сути то, что было получено в шаге 1, но уже распарсено из JSON-строки). Мы проходим по каждому ответу, собираем результаты. В summary считаем общее число ответов, суммируем оценки формальности, подсчитываем нарушения каждого типа. Затем сохраняем всё в файл style_eval.json с отступами (indent=2) и не ASCII-кодированием (чтобы кириллица читалась напрямую).

 
Пример содержимого style_eval.json (для двух условных ответов выше) мог бы быть таким:

{
  "answers": [
    {
      "id": 1,
      "text": "Здравствуйте! Я рад помочь. Вот решение...",
      "llm_scores": {
        "formality_score": 9,
        "no_first_person": false,
        "comment": "Стиль вежливый и формальный."
      },
      "violations": {
        "no_first_person": true,
        "too_many_sentences": false
      }
    },
    {
      "id": 2,
      "text": "Привет, думаю, тебе стоит попробовать это...",
      "llm_scores": {
        "formality_score": 4,
        "no_first_person": false,
        "comment": "Стиль слишком разговорный, обращение на 'ты'."
      },
      "violations": {
        "no_first_person": false,
        "too_many_sentences": false
      }
    }
  ],
  "summary": {
    "total_answers": 2,
    "violations_count": {
      "no_first_person": 1,
      "too_many_sentences": 0
    },
    "average_formality_score": 6.5
  }
}

                  
Здесь мы видим, например, что у ответа с id=1 no_first_person нарушение = true (т.е. обнаружено "Я рад помочь"), хотя модель указала "no_first_person": false (некоторое расхождение, повод уточнить критерии). Второй ответ получил низкий балл формальности и нарушил правило обращения на "ты" (LLM заметила, а наш линтер по no_first_person не считает это нарушением, ведь правило было только про "я"/"мы" – можно улучшить правила, добавив проверку "ты").

Требования к артефактам: Артефактом этого шага является сам файл style_eval.json – он должен быть прикреплён к уроку или заданию, согласно курсу. Кроме того, артефактом служит и скрипт, который мы написали выше, – его хорошо включить в репозиторий курса. Убедитесь, что:

Файл style_eval.json валидный JSON (проверьте открыв в редакторе или протестировав json.load).

Он содержит все требуемые поля, как указано в задании курса (если в курсе оговаривался формат, следуйте ему – например, может требоваться поле style_score суммарное и т.д.).

Скрипт генерации отчёта находится в понятном месте (например, tools/style_eval.py) и снабжён инструкцией по запуску (какие входные данные нужны – может быть, набор ответов из предыдущего задания).

Отчёт отражает действительно расширенный материал: если вы добавили дополнительные примеры и инструкции, упомянутые в задании, отразите их. Например, можно было добавить больше критериев в LLM-оценку (тональность, эмоциональность и др.) и расширить линтер (проверка обращения на "ты", проверка наличия цитирования источников и т.п. в зависимости от домена ответов). В отчёте тогда будет больше столбцов/полей. Это соответствует задаче урока – повторить и дополнительно проработать материал.
Шаг 4. Артефакт: скрипт генерации отчёта style_eval
Цель: Собрать воедино весь разработанный функционал (LLM-оценка, статические проверки, формирование JSON) в виде удобного скрипта или модуля. Этот скрипт является итоговым артефактом урока – его можно запустить на новом наборе ответов, чтобы получить style_eval.json, не проделывая всё вручную.

Объяснение: На предыдущих шагах мы писали код фрагментарно для демонстрации. Теперь нужно интегрировать его. Возможно, в рамках курса предполагается, что вы оформите это в виде Python-скрипта (файла .py) или ноутбука, который:

Берёт на вход данные – например, JSON или CSV с ответами, или подключается к предыдущей цепочке, генерирующей ответы.

Вызывает функции LLM-оценки и линтера для каждого ответа.

Сохраняет отчёт в файл.

Скрипт должен быть самодостаточным и удобным. Например, может быть сделан CLI-интерфейс (используя argparse), где параметры – путь к входному файлу с ответами и путь к выходному JSON. Либо если ответы известны (как часть конвейера), скрипт может быть жестко привязан к ним.

Инженерные приёмы:

Структура кода: Хорошей практикой будет оформить функции: evaluate_answer(answer_text) -> dict возвращает оценку стиля (объединяя LLM и правила для одного ответа). И функцию evaluate_all(answers: list) -> dict возвращающую весь отчёт (то, что мы строили в цикле). Тогда if __name__ == "__main__": блок может парсить аргументы, читать файл и вызывать evaluate_all, а затем писать файл.

Обработка ошибок: Продумайте, что если API LLM недоступно или вылетит ошибка в середине обработки (например, на 50-м из 100 ответов). Желательно, чтобы скрипт не потерял уже проделанную работу: можно сохранять промежуточные результаты каждые N ответов, или хотя бы обернуть вызов LLM в try/except и, например, пометить оценку как недоступную для этого ответа, продолжив работу. В отчёте можно добавить поле error для таких случаев.

Логи и прогресс: Если ответов много, выводите прогресс (например, “Evaluating answer 37/100...”), чтобы пользователь видел, что скрипт работает и на каком этапе. Это особенно актуально, если LLM-оценка длительная (помните, 100 вызовов модели могут занять несколько минут).

Fallback-модель: В задании указано использовать GPT-5 как основную модель и GPT-4o как запасную. Реализуйте это: если при обращении к GPT-5 случился таймаут или ошибка, можно автоматически попробовать вызвать GPT-4o. Это повысит надёжность. Например:

try:
    llm_response = openai.ChatCompletion.create(model="gpt-5", ...)
except Exception as e:
    print("Warning: GPT-5 failed, using GPT-4o. Error:", e)
    llm_response = openai.ChatCompletion.create(model="gpt-4o", ...)

                  
Тогда скрипт не упадёт целиком из-за временных проблем с GPT-5. Либо можно заранее оценивать: GPT-5 может быть дороже, и на большом числе запросов вы можете по бюджету решить часть сразу гнать на GPT-4o – это уже опционально.

Лучшие практики:

Повторное использование: Сделайте так, чтобы скрипт мог легко подключаться к другим частям системы. Например, если далее в курсе, в продакшене вы захотите регулярно мониторить стиль ответов, этот скрипт можно запускать в cron или CI. Для этого хорошо, чтобы он мог, например, брать свежие логи ответов откуда-то. Старайтесь не хардкодить данные – лучше параметризовать.

Чистый код: Разбейте на функции, избегайте дублирования (вынесите общие действия), названия переменных – говорящие. Помните, этот скрипт – демонстрация инженерного подхода.

Документация: В начале файла или в README к уроку опишите, как использовать артефакт. Например: “Чтобы сгенерировать отчёт по стилю, запустите python style_eval.py --input answers.json --output style_eval.json. Файл answers.json должен содержать список ответов для оценки в поле 'answers'.” – или что-то в этом роде, соответствуя формату ваших данных.

Пример кода: Ниже приведён условный каркас итогового скрипта (функции evaluate_answer и main). Он объединяет всё, что мы обсуждали:

 
import json, sys, argparse
import openai

# Инициализация EssayAI API (общая для всех вызовов)
openai.api_base = "https://api.essayai.ru/v1"
openai.api_key = "<API_KEY>"

def evaluate_answer(answer_text: str) -> dict:
    """Оценивает стиль одного ответа: возвращает словарь с LLM-оценкой и нарушениями правил."""
    # Вызов LLM для оценки
    try:
        response = openai.ChatCompletion.create(
            model="gpt-5",
            messages=[...],  # здесь подготовка сообщений как делали ранее
            timeout=10  # например, таймаут 10 сек
        )
    except Exception as e:
        # Fallback на GPT-4o
        response = openai.ChatCompletion.create(
            model="gpt-4o",
            messages=[...]
        )
    # Парсим ответ модели (ожидаем JSON-строку)
    eval_data = {}
    try:
        content = response['choices'][0]['message']['content']
        eval_data = json.loads(content)
    except Exception as e:
        eval_data = {"error": "LLM output parsing failed", "raw_content": content}

    # Применяем статические правила
    violations = {
        "no_first_person": not check_no_first_person(answer_text),
        "too_many_sentences": not check_sentence_count(answer_text, max_sentences=3)
        # ... другие правила при необходимости ...
    }
    return {"llm_scores": eval_data, "violations": violations}

def main(input_path: str, output_path: str):
    # Загружаем ответы
    with open(input_path, "r", encoding="utf-8") as f:
        data = json.load(f)
    answers = data.get("answers", data)  # допустим, входной JSON либо список, либо {"answers": [...]} 

    report = {"answers": [], "summary": {}}
    violations_count = {"no_first_person": 0, "too_many_sentences": 0}
    formality_score_sum = 0
    total = len(answers)

    for idx, ans in enumerate(answers, start=1):
        text = ans["text"] if isinstance(ans, dict) else str(ans)
        result = evaluate_answer(text)
        entry = {
            "id": ans.get("id", idx),
            "text": text,
            "llm_scores": result["llm_scores"],
            "violations": result["violations"]
        }
        report["answers"].append(entry)
        # обновляем счётчики
        if result["violations"]["no_first_person"]:
            violations_count["no_first_person"] += 1
        if result["violations"]["too_many_sentences"]:
            violations_count["too_many_sentences"] += 1
        score = result["llm_scores"].get("formality_score")
        if isinstance(score, (int, float)):
            formality_score_sum += score

        print(f"[{idx}/{total}] Answer {entry['id']} evaluated")  # прогресс

    # Заполняем summary
    report["summary"]["total_answers"] = total
    report["summary"]["violations_count"] = violations_count
    report["summary"]["average_formality_score"] = round(formality_score_sum/total, 2) if total else None

    # Сохраняем отчет
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(report, f, ensure_ascii=False, indent=2)
    print(f"Saved style evaluation report to {output_path}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Evaluate style of answers and generate report.")
    parser.add_argument("--input", "-i", required=True, help="Path to input JSON with answers.")
    parser.add_argument("--output", "-o", default="style_eval.json", help="Path to output report JSON.")
    args = parser.parse_args()
    main(args.input, args.output)

                  
Комментарий: Этот скрипт обобщён. Он читает входной файл, в котором ожидается либо JSON с ключом "answers" (список ответов), либо просто список. Затем для каждого ответа вызывает evaluate_answer (которая внутри обращается к EssayAI для LLM-оценки и к нашим правилам). Результаты собираются, считается сводная статистика, и всё сохраняется. Мы добавили некоторый вывод прогресса в консоль и обработку ошибок LLM. Конечно, здесь опущены детали формирования сообщений (messages=[...]) для вызова модели – вы должны вставить сюда свой промпт на основе того, что делали в Шаге 1 (с критериями стиля).

Требования к артефактам:

Скрипт оценки стиля: должно быть понятно, как его запустить. Если по заданию нужно приложить его код, отформатируйте красиво, не забудьте импортировать/определить все функции (у нас, например, check_no_first_person и др., они должны присутствовать).

Отчёт style_eval.json: убедитесь, что файл генерируется и содержит правдоподобные данные. В идеале – протестируйте скрипт на нескольких примерах ответов, визуально проверьте JSON.

Дополнительные примеры и инструкции: поскольку в задании явно указано "повтори и дополнительно проработай материал, с расширенными примерами", не стесняйтесь добавить в отчёт или вывод скрипта что-то сверх минимума. Например, можно расширить LLM-промпт для оценки, добавив больше критериев (тональность, грамматика, эмоциональная окраска), и вывести их в отчёт. Или подключить библиотеку проверки грамматики и добавить поле grammar_errors в violations. Все такие улучшения демонстрируют ваше понимание темы и придадут артефакту ценность.

Task:
Изучи контекст выше, дай необходимые артифакты и код их генрации, там где это требуется. 

Не пиши не каких примеров. Только, то, что тебя попросили. 


Constraint: 
    - отвечай только на тот вопрос, что тебя попросили 
    - не добавляй в ответ, ничего лишнего 
    - не создавай примеров использования, readme и прочего, если тебя явно об этом не попросили
    - НИКОГДА не пиши демонстрационных примеров, примеров использования, инструкций и readme, пока я сам тебя об этом не попрошу 
